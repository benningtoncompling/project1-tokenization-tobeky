
  
    Wikipedia
    enwiki
    httpsen.wikipedia.orgwikiMainPage
    MediaWiki 1.33.0wmf.18
    firstletter
    
      Media
      Special
      
      Talk
      User
      User talk
      Wikipedia
      Wikipedia talk
      File
      File talk
      MediaWiki
      MediaWiki talk
      Template
      Template talk
      Help
      Help talk
      Category
      Category talk
      Portal
      Portal talk
      Book
      Book talk
      Draft
      Draft talk
      Education Program
      Education Program talk
      TimedText
      TimedText talk
      Module
      Module talk
      Gadget
      Gadget talk
      Gadget definition
      Gadget definition talk
    
  
  
    Lexical analysis
    0
    81251
    
      884475029
      872271284
      20190221T214201Z
      
        Citation bot
        7903804
      
      
      Removed parameters.  You can WPUCBuse this bot yourself. WPDBUGReport bugs here.  WPUCBUseractivated.
      wikitext
      textxwiki
      redirectLexerpeople with this nameLexer surname
In computer science, '''lexical analysis''', '''lexing''' or '''tokenization''' is the process of converting a sequence of characters such as in a computer program or web page into a sequence of tokens String computer sciencestrings with an assigned and thus identified meaning. A program that performs lexical analysis may be termed a ''lexer'', ''tokenizer'',ltrefgtcite weburlhttpwww.cs.man.ac.uk~pjjfarrellcomp3.htmltitleAnatomy of a Compiler and The Tokenizerwebsitewww.cs.man.ac.ukltrefgt or ''scanner'', though ''scanner'' is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the Syntax programming languagessyntax of programming languages, web pages, and so forth.

 Applications 
A lexer forms the first phase of a compiler frontend in modern processing. Analysis generally occurs in one pass.

In older languages such as ALGOL, the initial stage was instead line reconstruction, which performed stropping syntaxunstropping and removed whitespace and Comment computer programmingcomments and had scannerless parsers, with no separate lexer. These steps are now done as part of the lexer.

Lexers and parsers are most often used for compilers, but can be used for other computer language tools, such as prettyprinters or Lint softwarelinters. Lexing can be divided into two stages the ''scanning'', which segments the input string into syntactic units called ''lexemes'' and categorizes these into token classes and the ''evaluating'', which converts lexemes into processed values.

Lexers are generally quite simple, with most of the complexity deferred to the parser or Semantic analysis compilerssemantic analysis phases, and can often be generated by a #Lexer generatorlexer generator, notably Lex softwarelex or derivatives. However, lexers can sometimes include some complexity, such as #Phrase structurephrase structure processing to make input easier and simplify the parser, and may be written partly or fully by hand, either to support more features or for performance.

 Lexeme 
A ''lexeme'' is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token.ltref nameautogtpage 111, Compilers Principles, Techniques, amp Tools, 2nd Ed. WorldCat by Aho, Lam, Sethi and Ullman, as quoted in httpsstackoverflow.comquestions14954721whatisthedifferencebetweentokenandlexemeltrefgt

Some authors term this a token, using token interchangeably to represent the string being tokenized, and the token data structure resulting from putting this string through the #Tokenizationtokenization process.ltrefgtcite web urlhttpperldoc.perl.orgperlinterp.html#Parsing titleperlinterp Perl 5 version 24.0 documentation authorPerl 5 Porters websiteperldoc.perl.org  Official documentation for the Perl programming language publisherperldoc.perl.org accessdate26 January 2017ltrefgtltrefgtcite web urlhttpsstackoverflow.comquestions14954721whatisthedifferencebetweentokenandlexeme#comment2099937114958865 titleWhat is the difference between token and lexeme? authorGuy Coder date19 February 2013 websiteStack Overflow publisherStack Exchange Inc accessdate26 January 2017ltrefgt

The word lexeme in computer science is defined differently than lexeme in linguistics. A lexeme in computer science roughly corresponds to what might be termed a word in linguistics the term Word computer architectureword in computer science has a different meaning than word in linguistics, although in some cases it may be more similar to a morpheme.

 Token 
ltLexical token and Token parser and Tokenize and Tokenizing redirect here MOSHEADgt
A ''lexical token'' or simply ''token'' is a String computer sciencestring with an assigned and thus identified meaning. It is structured as a pair consisting of a ''token name'' and an optional ''token value''. The token name is a category of lexical unit.ltref nameautogt Common token names are
 identifier names the programmer chooses
 keyword names already in the programming language
 separator also known as punctuators punctuation characters and paireddelimiters
 operator symbols that operate on arguments and produce results
 literal numeric, logical, textual, reference literals
 comment line, block.

classwikitable
 Examples of token values
 Token name  Sample token values

 identifier    codex, codecolor, codeUP

 keyword       code2cif, code2cwhile, code2creturn

 separator     ltcodegtltcodegt, ltcodegtltcodegt, ltcodegtltcodegt

 operator      code2c1, code2c1lt, code2c1

 literal       code2ctrue, code2c6.02e23, code2cmusic

 comment       code2c Retrieves user data , code2c must be negative


Consider this expression in the C programming languageC programming language
 code2c1x  a  b  2

The lexical analysis of this expression yields the following sequence of tokens
 ltcodegtidentifier, x, operator, , identifier, a, operator, , identifier, b, operator, , literal, 2, separator, ltcodegt

A token name is what might be termed a part of speech in linguistics.

 Lexical grammar 
furtherLexical grammar
The specification of a programming language often includes a set of rules, the lexical grammar, which defines the lexical syntax. The lexical syntax is usually a regular language, with the grammar rules consisting of regular expressions they define the set of possible character sequences lexemes of a token. A lexer recognizes strings, and for each kind of string found the lexical program takes an action, most simply producing a token.

Two important common lexical categories are Whitespace characterwhite space and Comment computer programmingcomments. These are also defined in the grammar and processed by the lexer, but may be discarded not producing any tokens and considered ''nonsignificant'', at most separating two tokens as in ltcodegtifampnbspxltcodegt instead of ltcodegtifxltcodegt. There are two important exceptions to this. First, in offside rule languages that delimit Block programmingblocks with indenting, initial whitespace is significant, as it determines block structure, and is generally handled at the lexer level see #Phrase structurephrase structure, below. Secondly, in some uses of lexers, comments and whitespace must be preserved – for examples, a prettyprinter also needs to output the comments and some debugging tools may provide messages to the programmer showing the original source code. In the 1960s, notably for ALGOL, whitespace and comments were eliminated as part of the line reconstruction phase the initial phase of the compiler frontend, but this separate phase has been eliminated and these are now handled by the lexer.

 Tokenization 
''Tokenization'' is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing. The process can be considered a subtask of parsing input.

Note ''Tokenization data securityTokenization'' in the field of computer security has a different meaning.

For example, in the text String computer sciencestring
 ltcodegtThe quick brown fox jumps over the lazy dogltcodegt

the string isn't implicitly segmented on spaces, as a natural language speaker would do. The raw input, the 43 characters, must be explicitly split into the 9 tokens with a given space delimiter i.e., matching the string ltcodegt ltcodegt or regular expression ltcodegt\s1ltcodegt.

The tokens could be represented in XML,

ltsyntaxhighlight source langxmlgt
ltsentencegt
  ltwordgtTheltwordgt
  ltwordgtquickltwordgt
  ltwordgtbrownltwordgt
  ltwordgtfoxltwordgt
  ltwordgtjumpsltwordgt
  ltwordgtoverltwordgt
  ltwordgttheltwordgt
  ltwordgtlazyltwordgt
  ltwordgtdogltwordgt
ltsentencegt
ltsyntaxhighlightgt

Or as an sexpression,

ltsyntaxhighlight source langschemegt
 sentence
   word The
   word quick
   word brown 
   word fox
   word jumps
   word over 
   word the
   word lazy
   word dog
ltsyntaxhighlightgt

When a token class represents more than one possible lexeme, the lexer often saves enough information to reproduce the original lexeme, so that it can be used in Semantic analysis compilerssemantic analysis. The parser typically retrieves this information from the lexer and stores it in the abstract syntax tree. This is necessary in order to avoid information loss in the case of numbers and identifiers.

Tokens are identified based on the specific rules of the lexer. Some methods used to identify tokens include regular expressions, specific sequences of characters termed a Flag computingflag, specific separating characters called delimiters, and explicit definition by a dictionary. Special characters, including punctuation characters, are commonly used by lexers to identify tokens because of their natural use in written and programming languages.

Tokens are often categorized by character content or by context within the data stream. Categories are defined by the rules of the lexer. Categories often involve grammar elements of the language used in the data stream. Programming languages often categorize tokens as identifiers, operators, grouping symbols, or by data type. Written languages commonly categorize tokens as nouns, verbs, adjectives, or punctuation. Categories are used for postprocessing of the tokens either by the parser or by other functions in the program.

A lexical analyzer generally does nothing with combinations of tokens, a task left for a parser. For example, a typical lexical analyzer recognizes parentheses as tokens, but does nothing to ensure that each  is matched with a .

When a lexer feeds tokens to the parser, the representation used is typically an enumerated list of number representations. For example, Identifier is represented with 0, Assignment operator with 1, Addition operator with 2, etc.

Tokens are defined often by regular expressions, which are understood by a lexical analyzer generator such as lex softwarelex. The lexical analyzer generated automatically by a tool like lex, or handcrafted reads in a stream of characters, identifies the #Lexemelexemes in the stream, and categorizes them into tokens. This is termed ''tokenizing''. If the lexer finds an invalid token, it will report an error.

Following tokenizing is parsing. From there, the interpreted data may be loaded into data structures for general use, interpretation, or compiling.

 Scanner 
The first stage, the ''scanner'', is usually based on a finitestate machine FSM. It has encoded within it information on the possible sequences of characters that can be contained within any of the tokens it handles individual instances of these character sequences are termed #Lexemelexemes. For example, an ''integer'' token may contain any sequence of numerical digit characters.  In many cases, the first nonwhitespace character can be used to deduce the kind of token that follows and subsequent input characters are then processed one at a time until reaching a character that is not in the set of characters acceptable for that token this is termed the ''maximal munch'', or ''longest match'', rule. In some languages, the lexeme creation rules are more complex and may involve backtracking over previously read characters. For example, in C, one 'L' character is not enough to distinguish between an identifier that begins with 'L' and a widecharacter string literal.

 Evaluator 
A #Lexemelexeme, however, is only a string of characters known to be of a certain kind e.g., a string literal, a sequence of letters. In order to construct a token, the lexical analyzer needs a second stage, the ''evaluator'', which goes over the characters of the lexeme to produce a ''value''. The lexeme's type combined with its value is what properly constitutes a token, which can be given to a parser. Some tokens such as parentheses do not really have values, and so the evaluator function for these can return nothing only the type is needed. Similarly, sometimes evaluators can suppress a lexeme entirely, concealing it from the parser, which is useful for whitespace and comments. The evaluators for identifiers are usually simple literally representing the identifier, but may include some stropping syntaxunstropping. The evaluators for integer literals may pass the string on deferring evaluation to the semantic analysis phase, or may perform evaluation themselves, which can be involved for different bases or floating point numbers. For a simple quoted string literal, the evaluator needs to remove only the quotes, but the evaluator for an String literal#Escape sequencesescaped string literal incorporates a lexer, which unescapes the escape sequences.

For example, in the source code of a computer program, the string
  code2c1networthfuture  assets  liabilities
might be converted into the following lexical token stream whitespace is suppressed and special characters have no value
 IDENTIFIER networthfuture
 EQUALS
 OPENPARENTHESIS
 IDENTIFIER assets
 MINUS
 IDENTIFIER liabilities
 CLOSEPARENTHESIS
 SEMICOLON

Though it is possible and sometimes necessary, due to licensing restrictions of existing parsers or if the list of tokens is small, to write a lexer by hand, lexers are often generated by automated tools. These tools generally accept regular expressions that describe the tokens allowed in the input stream. Each regular expression is associated with a Formal grammar#The syntax of grammarsproduction rule in the lexical grammar of the programming language that evaluates the lexemes matching the regular expression. These tools may generate source code that can be compiled and executed or construct a state transition table for a finitestate machine which is plugged into template code for compiling and executing.

Regular expressions compactly represent patterns that the characters in lexemes might follow. For example, for an English languageEnglishbased language, an IDENTIFIER token might be any English alphabetic character or an underscore, followed by any number of instances of ASCII alphanumeric characters andor underscores. This could be represented compactly by the string codeazAZazAZ09. This means any character az, AZ or , followed by 0 or more of az, AZ,  or 09.

Regular expressions and the finitestate machines they generate are not powerful enough to handle recursive patterns, such as ''n'' opening parentheses, followed by a statement, followed by ''n'' closing parentheses. They are unable to keep count, and verify that ''n'' is the same on both sides, unless a finite set of permissible values exists for ''n''. It takes a full parser to recognize such patterns in their full generality. A parser can push parentheses on a stack and then try to pop them off and see if the stack is empty at the end see exampleltrefgtcite weburlhttpmitpress.mit.edusicpfulltextbookbookZH31.html#%25sec5.1.4titleStructure and Interpretation of Computer Programswebsitemitpress.mit.edultrefgt in the ''Structure and Interpretation of Computer Programs'' book.

 Obstacles 
Typically, tokenization occurs at the word level. However, it is sometimes difficult to define what is meant by a word. Often a tokenizer relies on simple heuristics, for example
 Punctuation and whitespace may or may not be included in the resulting list of tokens.
 All contiguous strings of alphabetic characters are part of one token likewise with numbers.
 Tokens are separated by whitespace characterwhitespace characters, such as a space or line break, or by punctuation characters.

In languages that use interword spaces such as most that use the Latin alphabet, and most programming languages, this approach is fairly straightforward. However, even here there are many edge cases such as Poetic contractioncontractions, hyphenhyphenated words, emoticons, and larger constructs such as URIs which for some purposes may count as single tokens. A classic example is New Yorkbased, which a naive tokenizer may break at the space even though the better break is arguably at the hyphen.
 
Tokenization is particularly difficult for languages written in scriptio continua which exhibit no word boundaries such as Ancient Greek, Chinese languageChinese,ltrefgtHuang, C., Simon, P., Hsieh, S., amp Prevot, L. 2007 httpwww.aclweb.organthologyPP07P072018.pdf Rethinking Chinese Word Segmentation Tokenization, Character Classification, or Word break Identificationltrefgt or Thai languageThai. Agglutinative languages, such as Korean, also make tokenization tasks complicated.

Some ways to address the more difficult problems include developing more complex heuristics, querying a table of common specialcases, or fitting the tokens to a language model that identifies collocations in a later processing step.

 Software 
 httpopennlp.apache.orgindex.html Apache OpenNLP includes rule based and statistical tokenizers which support many languages
 httptokenizer.tool.uniwits.com UTokenizer is an API over HTTP that can cut Mandarin and Japanese sentences at word boundary.  English is supported as well.
 httpsdev.havenondemand.comapistokenizetext#overview HPE Haven OnDemand Text Tokenization API Commercial product, with freemium access uses Advanced Probabilistic Concept Modelling to determine the weight that the term holds in the specified text indexes
 The Lex softwareLex tool and its compiler is designed to generate code for fast lexical analysers based on a formal description of the lexical syntax. It is generally considered insufficient for applications with a complex set of lexical rules and severe performance requirements. For example, the GNU Compiler Collection GCC uses handwritten lexers.

 Lexer generator 
see alsoParser generator
Lexers are often generated by a ''lexer generator'', analogous to parser generators, and such tools often come together. The most established is Lex softwarelex, paired with the yacc parser generator, and the free equivalents flex lexical analyserflexbison. These generators are a form of domainspecific language, taking in a lexical specification – generally regular expressions with some markup – and emitting a lexer.

These tools yield very fast development, which is very important in early development, both to get a working lexer and because a language specification may change often. Further, they often provide advanced features, such as pre and postconditions which are hard to program by hand. However, an automatically generated lexer may lack flexibility, and thus may require some manual modification, or an allmanually written lexer.

Lexer performance is a concern, and optimizing is worthwhile, more so in stable languages where the lexer is run very often such as C or HTML. lexflexgenerated lexers are reasonably fast, but improvements of two to three times are possible using more tuned generators. Handwritten lexers are sometimes used, but modern lexer generators produce faster lexers than most handcoded ones. The lexflex family of generators uses a tabledriven approach which is much less efficient than the directly coded approach.Dubioustabledriven vs directly codeddateMay 2010lt The tabledriven approach is not the problem see 'control table' article  flex appears to be inefficient and is not using a trivial hash function. gt With the latter approach the generator produces an engine that directly jumps to followup states via goto statements. Tools like re2cltrefgtCite journal last1 Bumbulis first1 P. last2 Cowan first2 D. D. doi 10.1145176454.176487 title RE2C A more versatile scanner generator journal ACM Letters on Programming Languages and Systems volume 2 issue 1–4 pages 70–84 date Mar–Dec 1993ltrefgt have proven to produce engines that are between two and three times faster than flex produced engines.Citation neededdateApril 2008 It is in general difficult to handwrite analyzers that perform better than engines generated by these latter tools.

 List of lexer generators 
example farmsectiondateSeptember 2018
see alsoList of parser generators
 ANTLR – can generate lexical analyzers and parsers
 DFASTAR – generates DFA matrix tabledriven lexers in C
 Flex lexical analyserFlex – variant of the classic ''Lex softwarelex'' for CC
 Ragel – state machine and lexer generator with output in C, C, and Assembly
 re2c – lexer generator for C programming languageC and C

The following lexical analysers can handle Unicode
 JavaCC – generates lexical analyzers written in Java
 httpsgithub.comjflexdejflex JFLex – lexical analyzer generator for Java
 AnnoFlex  annotationbased code generator for lexical scanners for Java
 httpsgithub.comGeniviaREflex REflex  a fast variant of lexflex for C generates scanners with tables or direct code
 httpquex.sourceforge.net Quex – fast universal lexical analyzer generator for C and C written in Python
 FsLex – lexer generator for byte and Unicode character input for F#
 re2c – lexer generator for C programming languageC and Cltrefgthttpre2c.orgmanualfeaturesencodingsencodings.html, re2c manualltrefgt
 PLY Python LexYaccPLY   the Python module ply.lex enables the lexical analysis part

 Phrase structure 
Lexical analysis mainly segments the input stream of characters into tokens, simply grouping the characters into pieces and categorizing them. However, the lexing may be significantly more complex most simply, lexers may omit tokens or insert added tokens. Omitting tokens, notably whitespace and comments, is very common, when these are not needed by the compiler. Less commonly, added tokens may be inserted. This is done mainly to group tokens into Statement computer sciencestatements, or statements into blocks, to simplify the parser.

 Line continuation 
Line continuation is a feature of some languages where a newline is normally a statement terminator. Most often, ending a line with a backslash immediately followed by a newline results in the line being ''continued'' – the following line is ''joined'' to the prior line. This is generally done in the lexer the backslash and newline are discarded, rather than the newline being tokenized. Examples include Bash Unix shellbash,ltrefgt''httpswww.gnu.orgsoftwarebashmanualbashref.html Bash Reference Manual'', httpswww.gnu.orgsoftwarebashmanualbashref.html#EscapeCharacter 3.1.2.1 Escape Characterltrefgt other shell scripts and Python.ltref name3.6.4 Documentationgtcite weburlhttpsdocs.python.orgtitle3.6.4 Documentationwebsitedocs.python.orgltrefgt

 Semicolon insertion 
Many languages use the semicolon as a statement terminator. Most often this is mandatory, but in some languages the semicolon is optional in many contexts. This is mainly done at the lexer level, where the lexer outputs a semicolon into the token stream, despite one not being present in the input character stream, and is termed ''semicolon insertion'' or ''automatic semicolon insertion''. In these cases, semicolons are part of the formal phrase grammar of the language, but may not be found in input text, as they can be inserted by the lexer. Optional semicolons or other terminators or separators are also sometimes handled at the parser level, notably in the case of trailing commas or semicolons.

Semicolon insertion is a feature of BCPL and its distant descendent Go programming languageGo,ltrefgt''httpgolang.orgdoceffectivego.html Effective Go'', httpgolang.orgdoceffectivego.html#semicolons Semicolonsltrefgt though it is absent in B or C.ltrefgthttpsgroups.google.comforum#topicgolangnutsXuMrWI0Q8uk Semicolons in Go, golangnuts, Rob 'Commander' Pike, 121009ltrefgt Semicolon insertion is present in JavaScript, though the rules are somewhat complex and muchcriticized to avoid bugs, some recommend always using semicolons, while others use initial semicolons, termed defensive semicolons, at the start of potentially ambiguous statements.

Semicolon insertion in languages with semicolonterminated statements and line continuation in languages with newlineterminated statements can be seen as complementary semicolon insertion adds a token, even though newlines generally do ''not'' generate tokens, while line continuation prevents a token from being generated, even though newlines generally ''do'' generate tokens.

 Offside rule 
furtherOffside rule
The offside rule blocks determined by indenting can be implemented in the lexer, as in Python programming languagePython, where increasing the indenting results in the lexer emitting an INDENT token, and decreasing the indenting results in the lexer emitting a DEDENT token.ltref name3.6.4 Documentationgt These tokens correspond to the opening brace ltcodegtltcodegt and closing brace ltcodegtltcodegt in languages that use braces for blocks, and means that the phrase grammar does not depend on whether braces or indenting are used. This requires that the lexer hold state, namely the current indent level, and thus can detect changes in indenting when this changes, and thus the lexical grammar is not Contextfree grammarcontextfree INDENT–DEDENT depend on the contextual information of prior indent level.

 Contextsensitive lexing 
Generally lexical grammars are contextfree, or almost so, and thus require no looking back or ahead, or backtracking, which allows a simple, clean, and efficient implementation. This also allows simple oneway communication from lexer to parser, without needing any information flowing back to the lexer.

There are exceptions, however. Simple examples include semicolon insertion in Go, which requires looking back one token concatenation of consecutive string literals in Python,ltref name3.6.4 Documentationgt which requires holding one token in a buffer before emitting it to see if the next token is another string literal and the offside rule in Python, which requires maintaining a count of indent level indeed, a stack of each indent level. These examples all only require lexical context, and while they complicate a lexer somewhat, they are invisible to the parser and later phases.

A more complex example is the lexer hack in C, where the token class of a sequence of characters cannot be determined until the semantic analysis phase, since typedef names and variable names are lexically identical but constitute different token classes. Thus in the hack, the lexer calls the semantic analyzer say, symbol table and checks if the sequence requires a typedef name. In this case, information must flow back not from the parser only, but from the semantic analyzer back to the lexer, which complicates design.

 Notes 
Notelist

 References 
Reflist

Sources
refbegin
 ''Compiling with C# and Java'', Pat Terry, 2005, ISBN032126360X
 ''Algorithms  Data Structures  Programs'', Niklaus Wirth, 1975, ISBN0130224189
 ''Compiler Construction'', Niklaus Wirth, 1996, ISBN0201403536
 Sebesta, R. W. 2006. Concepts of programming languages Seventh edition pp.ampnbsp177. Boston PearsonAddisonWesley.
refend

 External links 
 cite journal url httppeople.cs.nctu.edu.tw~wuuyanghomepagepapersapplicability2002.ps first1 W. last1 Yang first2 CheyWoei last2 Tsay first3 JienTsai last3 Chan title On the applicability of the longestmatch rule in lexical analysis journal Computer Languages, Systems amp Structures volume 28 issue 3 pages 273ampndash288 date 2002 id NSC 862213E009021 and NSC 862213E009079 doi 10.1016S0096055102000140 
 cite web url httpswww.ibm.comdeveloperworkscommunityblogsnlpentrytokenization title The Art of Tokenization first Craig last Trim date Jan 23, 2013 publisher IBM work Developer Works
 httpwww.gabormelli.comRKBWordMentionSegmentationTask Word Mention Segmentation Task, an analysis

DEFAULTSORTLexical Analysis
CategoryCompiler construction
CategoryProgramming language implementation
CategoryParsing
      k7von5udcv6g3jh2y85uhb58pbww2qi
    
  

